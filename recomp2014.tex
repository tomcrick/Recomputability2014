\documentclass[conference]{IEEEtran}

\usepackage[british]{babel}
\usepackage[hyphens]{url}
\usepackage{graphicx}
\usepackage[noadjust]{cite}
\usepackage[pdftex,colorlinks=true]{hyperref}

\begin{document}
\bstctlcite{IEEEexample:BSTcontrol}

% still not happy with the title...
%\title{``Reproducibility-as-a-Service'': Dream or Reality?}
\title{On Usefully Publishing Scientific Models} 

% author names and affiliations
% use a multiple column layout for up to three different
% affiliations
\author{\IEEEauthorblockN{Tom Crick}
\IEEEauthorblockA{Department of Computing\\
Cardiff Metropolitan University\\
Cardiff, UK\\
Email: {\url{tcrick@cardiffmet.ac.uk}}}
\and
\IEEEauthorblockN{Benjamin A. Hall and Samin Ishtiaq and Kenji Takeda}
\IEEEauthorblockA{Microsoft Research\\
Cambridge, UK\\
Email: {\url{{benhall,samin.ishtiaq,kenji.takeda}@microsoft.com}}}}

\maketitle

\begin{abstract}
Abstract here...
\end{abstract}

\IEEEpeerreviewmaketitle

\section{Introduction- reproducibility in the compuational sciences}
Two key types of results arise from work done in the computational sciences; models 
and algorithms. Models represent an abstraction of reality, and their behaviour is
expected to reliably reproduced if different algorithms are used. This validation
of a model's behaviour can be impacted by a number of factors relating to the specific
techniques used, but similar approaches are expected to give the same broad results. 
In contrast, when new algorithms are proposed to replace or supplement existing 
algorithms, they are expected to verifiably replicate the results of other algorithms.

Neither class of result though exists in isolation; a new algorithm is dependent on 
a set of models (or benchmarks) to demonstrate its new capabilities. Equally, model 
development can both necessitate the development of new algorithms and highlight the 
differences between alternative approaches. Whilst algorithms and their implementations
have been highlighted as a potential barrier to reproducibility~\cite{crick-et-al_wssspe2},
here we discuss the value of improved access and sharing of models in avoiding mistakes
and in generating new scientific insights. We describe efforts to reproduce computational 
models and algorithms, specifically the many issues related to benchmarks of models and 
algorithms. We conclude with thoughts on where efforts should be fcussed in both the short- 
and long-term to move to a world in which computational reproducibility helps researchers 
achieve their goals, rather than being perceived as an overhead.

% there are a lot of Rs in this domain:
% Replacing the Paper: The Twelve Rs of the e-Research Record~\cite{deroure:2010}
% http://www.scilogs.com/eresearch/replacing-the-paper-the-twelve-rs-of-the-e-research-record/

\section{Motivation}
The reproduction and replication of reported scientific results is a
widely discussed topic within the scientific community~\cite{barnes:2010,morin-et-al:2012,joppa-et-al:2013}. 
Whilst the retraction of several studies has drawn the focus of many
commenters, automated systems, which allow easy reproduction of results, offer the 
potential to improve the efficiency of scientific exploration and drive the
adoption of new techniques. 

New tools for membrane protein simulation~\cite{Stansfeld,hall2014} demonstrate
how complex workflows can be automated, preventing errors and differences 
arising from manual execution, whilst making it faster to perform new 
analyses. 
Alongside this, recent work in executable biology~\cite{Cook} showed how
a new class of models, representing a defined property of biological networks,
defeated an existing algorithm for proving stability. As such, it was the 
broader application of a new algorithm to additional models (or benchmarks)
which highlighted an unexplored but important phenomena the algorithm could 
not address.

Whilst experimental work requires detailed protocol descriptions, computer
science and computational science disciplines uniquely are
able to share the raw outputs of their work as software and standard format datafiles. Despite
this advantage, and alongside ongoing -- and significant -- changes to the
traditional models of academic dissemination and
publication~\cite{deroure:2010,stodden-et-al:2013,fursin+dubach:2014},
there remain cultural and technical barriers to both the
sharing and reimplementation of
algorithms~\cite{crick-et-al_wssspe2}. These include widely discussed
topics, such as choices of programming language and licence, as well
as unreported technical details of the implementations and ensuring
that developers get credit for their work.

% further to #overlyhonestmethods...
% http://www.phdcomics.com/comics.php?f=1689

However, even when this is considered, the testing of the benchmarks
themselves may be non-trivial. Benchmarks may be tailored to the 
specific problems being addressed by the algorithm, and may not be 
expected to cover all edge cases. Implementation details, such as 
floating point rounding behaviour may change benchmark results.
Furthermore, for high performance computing applications, repeating
benchmark results may not be possible by other groups without equivalent
resources. Here we extend a previous proposal~\cite{crick-et-al_wssspe2}
to specifically discuss the role of benchmark sets and analysis.

Quote: ``at least 50\% of published studies, even those in top-tier
academic journals, can't be repeated with the same conclusions by an
industrial lab''~\cite{osherovich:2011}.

%In the cloud world, we have IaaS, PaaS and SaaS...do we need RaaS: ``Reproducibility-as-a-Service''?

%SI/Ben to work on this

\section{Roads to reproducibility}

% A section about the ``How'', and the barriers to the How. 

Need to inline these examples through out the section: 
	c.f. Codalab, Kaggle

	e.g. real-world validation - AIAA drag benchmarks, ERCOFTAC
	Possibly CAPRI/CASP? (protein interaction and folding competitions)


\subsection{Implementation or model?}
%Tom's section?

Specify details such as rounding behaviour in the model.

Include other examples of how abstract (vs concrete) the model should/could be. 

% Explicitly a ``protocol'' â€“ explicit steps, copy-paste, customisable,
% versioned; not black box.

% No requirement for computational expertise or significant
% computational hardware.


\subsection{Protocols as scripts}

'protocol' = 'work-flow'. 

Benchmarking is not always as trivial as grab some code, grab the 
existing data, and run. Data format conversions may be non-trivial
or performed manually. These may involve external scripts, which 
might not be part of any of the explicitly shared code. Open 
protocols may aid the reproducibility of benchmark results. Even 
better would be tools which aid the transformation of text based
protocols into real scripts.

Eg Codalab

Ben's example: get original model, do energy minimiztion, do some other transform, do energy minimization again. 

electronic lab books. Can't delete any of your notes. Oxford structural genomics labbooks. 

\subsection{Performance and scalability}
%Kenji to work on this
One big target for HPC users is scalability of code (e.g. gromacs,
NAMD, Desmond). Here the aim is to make simulations run more efficiently
over large numbers of cores/nodes. Scaling results however may be 
hard to reproduce without access to significant resource.

If speed isn't being explicitly tested, on-demand cloud resources 
may offer a route to testing small systems. Slow VMs in the cloud
may test scalability properties of algorithm without requiring access
to always on-resource.

SI: I wonder also if cloud providers can give some metric of what
load/latency/resources your job was run under.  That way, you could
calculate how your performance would scale up had you access to the
full resources. 


\subsection{Benchmark repositories and annotation}

%Kenji to work on this

Benchmarks as unit tests, stored in central repositories. 
Treat benchmark sets as a curated set of unit tests

Eg, SMTComp, SMT Lib. 

e.g. real-world validation - AIAA drag benchmarks, ERCOFTAC
	Possibly CAPRI/CASP? (protein interaction and folding competitions)

An API for benchmarks? 

Perhaps cite Fursin et al.~\cite{fursin-et-al:2014} here as an example?

Tom: 
example of memory profiling of CLP programs. Specialist solvers versus 
generic solvers SMT have papers run on different benchmarks, testing different 
things. 

\section{Continuous integration for models}

%Tom?

>>>>>>> Tag people
Code interacts with benchmarks in a reliable way. We want to share
code, and have proposed a mechanism for doing this~\cite{crick-et-al_wssspe2}.
Explicitly link online repositories of code to benchmark sets and
results. Allows reproduction of benchmark results from a paper, 
but also trivial expansion of benchmarking by linking to other 
available datasets.

\subsection{Composibility and unit testing}
%Tom?
In computer science, ``composability'' is a major concept. You are
meant to be able to put parts together. For instance, you should be
able to prove a property of X and of Y separately, and when you
combine X and Y, $X \oplus Y$, in some way, you should preserve that
property.  This is not the case in my other fields like machine
learning, computational science etc. Often, the nature of the problem
is that it's not understood how to decompose. Decomposability 
enables unit testing. 



\subsection{How to assess performance?}

%Kenji to work on this
Wallclock vs op count

Samin: Other structural properties like termination, stability, etc. 

\section{Future outlook}

A system as described here has several up front benefits- it link papers 
more closely to their outputs, making external validation easier and 
allowing interested users to explore unaddressed sets of models. These 
features alone will be expected to accelerate the research cycle, with
major benefits. \emph{4th paradigm}.

Limitations presented by big data- will some models resist validation?

Critical success factor is providing capabilities that help researchers 
in their day-to-day work, rather than being an overhead incurred, thus 
providing both immediate and long-term returns on time to the individual.

% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
% \IEEEtriggeratref{37}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}


% BibTeX users
\bibliographystyle{IEEEtran}
\bibliography{recomp2014}

\end{document}
