\documentclass[conference]{IEEEtran}

\usepackage[british]{babel}
\usepackage[hyphens]{url}
\usepackage{graphicx}
\usepackage[noadjust]{cite}
\usepackage[pdftex,colorlinks=true]{hyperref}

\begin{document}
\bstctlcite{IEEEexample:BSTcontrol}

% still not happy with the title...
\title{``Reproducibility-as-a-Service'': Dream or Reality?}

% author names and affiliations
% use a multiple column layout for up to three different
% affiliations
\author{\IEEEauthorblockN{Tom Crick}
\IEEEauthorblockA{Department of Computing\\
Cardiff Metropolitan University\\
Cardiff, UK\\
Email: {\url{tcrick@cardiffmet.ac.uk}}}
\and
\IEEEauthorblockN{Kenji Takeda, Benjamin A. Hall and Samin Ishtiaq}
\IEEEauthorblockA{Microsoft Research\\
Cambridge, UK\\
Email: {\url{{kenji.takeda,benhall,samin.ishtiaq}@microsoft.com}}}}

\maketitle

\begin{abstract}
Abstract here...
\end{abstract}

\IEEEpeerreviewmaketitle

\section{Introduction}

% there are a lot of Rs in this domain:
% Replacing the Paper: The Twelve Rs of the e-Research Record~\cite{deroure:2010}
% http://www.scilogs.com/eresearch/replacing-the-paper-the-twelve-rs-of-the-e-research-record/
The reproduction and replication of reported scientific results is a
topic of growing concern within the scientific
community~\cite{barnes:2010,morin-et-al:2012,joppa-et-al:2013}. Whilst
experimental work requires detailed protocol descriptions, computer
science and computational science disciplines uniquely are
able to share the raw outputs of their work- the software. Despite
this advantage, and alongside ongoing -- and significant -- changes to the
traditional models of academic dissemination and
publication~\cite{deroure:2010,stodden-et-al:2013,fursin+dubach:2014},
there remain cultural and technical barriers to both the
sharing and reimplementation of
algorithms~\cite{crick-et-al_wssspe2}. These include widely discussed
topics, such as choices of programming language and licence, as well
as unreported technical details of the implementations and ensuring
that developers get credit for their work.

% further to #overlyhonestmethods...
% http://www.phdcomics.com/comics.php?f=1689

However, even when this is considered, the testing of the benchmarks
themselves may be non-trivial. Benchmarks may be tailored to the 
specific problems being addressed by the algorithm, and may not be 
expected to cover all edge cases. Implementation details, such as 
floating point rounding behaviour may change benchmark results.
Furthermore, for high performance computing applications, repeating
benchmark results may not be possible by other groups without equivalent
resources. Here we extend a previous proposal~\cite{crick-et-al_wssspe2}
to specifically discuss the role of benchmark sets and analysis.

Quote: ``at least 50\% of published studies, even those in top-tier
academic journals, can't be repeated with the same conclusions by an
industrial lab''~\cite{osherovich:2011}.

In the cloud world, we have IaaS, PaaS and SaaS...do we need RaaS:
``Reproducibility-as-a-Service''?

\section{Implementation or model?}

Specify details such as rounding behaviour in the model

% Explicitly a ``protocol'' â€“ explicit steps, copy-paste, customisable,
% versioned; not black box.

% No requirement for computational expertise or significant
% computational hardware.


\section{Protocols as scripts}

Benchmarking is not always as trivial as grab some code, grab the 
existing data, and run. Data format conversions may be non-trivial
or performed manually. These may involve external scripts, which 
might not be part of any of the explicitly shared code. Open 
protocols may aid the reproducibility of benchmark results. Even 
better would be tools which aid the transformation of text based
protocols into real scripts.


\section{Performance and scalability}

One big target for HPC users is scalability of code (e.g. gromacs,
NAMD, Desmond). Here the aim is to make simulations run more efficiently
over large numbers of cores/nodes. Scaling results however may be 
hard to reproduce without access to significant resource.

If speed isn't being explicitly tested, on-demand cloud resources 
may offer a route to testing small systems. Slow VMs in the cloud
may test scalability properties of algorithm without requiring access
to always on-resource. \emph{NOTE- DOES THIS SOUND SENSIBLE?}


\section{Benchmark repositories and annotation}

Benchmarks as unit tests, stored in central repositories. An API 
for benchmarks? Treat benchmark sets as a curated set of unit tests

Perhaps cite Fursin et al.~\cite{fursin-et-al:2014} here as an example?

\section{Benchmarks as a service}

Code interacts with benchmarks in a reliable way. We want to share
code, and have proposed a mechanism for doing this~\cite{crick-et-al_wssspe2}.
Explicitly link online repositories of code to benchmark sets and
results. Allows reproduction of benchmark results from a paper, 
but also trivial expansion of benchmarking by linking to other 
available datasets.


% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
% \IEEEtriggeratref{37}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}


% BibTeX users
\bibliographystyle{IEEEtran}
\bibliography{recomp2014}

\end{document}
