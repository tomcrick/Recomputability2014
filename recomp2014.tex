\documentclass[conference]{IEEEtran}

\usepackage[british]{babel}
\usepackage[hyphens]{url}
\usepackage{graphicx}
\usepackage[noadjust]{cite}
\usepackage[pdftex,colorlinks=true]{hyperref}
\IEEEoverridecommandlockouts

\begin{document}
\bstctlcite{IEEEexample:BSTcontrol}

% still not happy with the title...
%\title{``Reproducibility-as-a-Service'': Dream or Reality?}
%\title{On Usefully Publishing Scientific Models} 
%\title{Learning to share: usefully publishing scientific models}
%\title{Learning to share: publishing useful scientific models}
\title{``Share and Enjoy'': Publishing Useful and Usable Scientific Models}

% author names and affiliations
% use a multiple column layout for up to three different
% affiliations
\author{\IEEEauthorblockN{Tom Crick}
\IEEEauthorblockA{Department of Computing\\
Cardiff Metropolitan University\\
Cardiff, UK\\
Email: {\url{tcrick@cardiffmet.ac.uk}}}%
\thanks{The authors would like to acknowledge the use of the Sirius Cybernetics
  Corporation's motto for the title of this paper. }
\and
\IEEEauthorblockN{Benjamin A. Hall and Samin Ishtiaq and Kenji Takeda}
\IEEEauthorblockA{Microsoft Research\\
Cambridge, UK\\
Email: {\url{{benhall,samin.ishtiaq,kenji.takeda}@microsoft.com}}}}

\maketitle

\begin{abstract}
Abstract here...
\end{abstract}

\IEEEpeerreviewmaketitle

\section{Introduction: reproducibility in the computational sciences}
Two key types of results arise from work done in the computational
sciences: models and algorithms. Models represent an abstraction of
reality, and their behaviour is expected to be reliably reproduced if
different algorithms are used. This validation of a model's behaviour
can be impacted by a number of factors relating to the specific
techniques used, but similar approaches are expected to give broadly
the same results.  In contrast, when new algorithms are proposed to
replace or supplement existing algorithms, they are expected to
verifiably replicate the results of other algorithms.

However, neither class of result exists in isolation: a new algorithm is
dependent on a set of models (or benchmarks) to demonstrate its new
capabilities. Equally, model development can both necessitate the
development of new algorithms and highlight the differences between
alternative approaches. Whilst algorithms and their implementations
have been highlighted as a potential barrier to
reproducibility~\cite{crick-et-al_wssspe2}, in this paper we discuss the value
of improved access and sharing of models in avoiding mistakes and in
generating new scientific insights. We describe efforts to reproduce
computational models and algorithms, specifically the multitude of issues
related to benchmarks of models and algorithms. 
We conclude with thoughts % SI: are we doing this yet? 
on where efforts should be focused in both the short- and
long-term to move to a world in which computational reproducibility
helps researchers achieve their goals, rather than being perceived as
an overhead.


\section{Motivation}
The reproduction and replication of reported scientific results is a
widely discussed topic within the scientific
community~\cite{barnes:2010,morin-et-al:2012,joppa-et-al:2013}.
Whilst the retraction of several studies has drawn the focus of many
commenters, automated systems, which allow easy reproduction of
results, offer the potential to improve the efficiency of scientific
exploration and drive the adoption of new techniques. Nevertheless,
this is wider socio-cultural problem that pervades the scientific
community, with estimates that as much as 50\% of published studies,
even those in top-tier academic journals, cannot be repeated with the
same conclusions by an industrial lab~\cite{osherovich:2011}.

New tools for membrane protein simulation~\cite{Stansfeld,Hall2014}
demonstrate how complex workflows can be automated, preventing errors
and differences arising from manual execution, whilst making it faster
to perform new analyses.  Alongside this, recent work in executable
biology~\cite{Cook} showed how a new class of models, representing a
defined property of biological networks, defeated an existing
algorithm for proving stability. As such, it was the broader
application of a new algorithm to additional models (or benchmarks)
which highlighted an unexplored but important phenomena the algorithm
could not address.

Whilst experimental work requires detailed protocol descriptions,
computer science and the computational science disciplines uniquely
are able to share the raw outputs of their work as software and
standard format datafiles. Despite this advantage, and alongside
ongoing -- and significant -- changes to the traditional models of
academic dissemination and
publication~\cite{deroure:2010,stodden-et-al:2013,fursin+dubach:2014},
there remain cultural and technical barriers to both the sharing and
reimplementation of algorithms~\cite{crick-et-al_wssspe2}. These
include widely discussed topics, such as choices of programming
language and licence, as well as unreported technical details of the
implementations and ensuring that developers get credit for their
work~\footnote{e.g. UK Community of Research Software Engineers:
  ~\url{http://www.rse.ac.uk/}}.

% further to #overlyhonestmethods...
% http://www.phdcomics.com/comics.php?f=1689

\begin{figure}[!ht]
\centering
\includegraphics[width=\columnwidth]{phd031214s.png}
\caption{Reproducible research\newline [source: \url{http://www.phdcomics.com/comics.php?f=1689}]}
\label{fig:reprodres} 
\end{figure}

However, even when this is considered, the testing of the benchmarks
themselves may be non-trivial. Benchmarks may be tailored to the
specific problems being addressed by the algorithm, and may not be
expected to cover all edge cases. Implementation details, such as
floating point rounding behaviour, may affect benchmark results.
Furthermore, for high performance computing applications, repeating
benchmark results may not be possible by other groups without
equivalent hardware resources. Here we extend a previous
proposal~\cite{crick-et-al_wssspe2} to specifically discuss the role
of benchmark sets and analysis.

%SI/Ben to work on this

\section{Roads to reproducibility}

% e.g. https://guides.github.com/activities/citable-code/

\subsection{Implementation or model?}

Published models require basic information to allow the models to be
explored. On the most basic level, this can include techniques to allow
files to be parsed, but in this parsing errors may arise from details 
stored in the original implementations of the model analysis algorithms.
A simple example arises from the treatment of floating point conversions
in qualitative networks in systems biology~\cite{Schaub2007}. Each
variable in a network has an algebraic target function which describes
how the variable should change at each step. The variables themselves are
integers, and the target function may return a float, which must be converted
to an integer for the update. This can be done in the target function 
itself, but if the function returns a float the specific implementation
dictates if this is a round, floor or ceil function. As such, this behaviour
from the implementation may change the results of the modelling.

An alternative example showing how the implementation of qualitative networks
may change the model is the treatment of variable ranges within the model.
Whilst the formalism allows the variables in a model to have differing 
ranges of integers, the mechanism of conversion is not specified in the 
formalism. As such, this is another area where implementations can dictate
the precise behaviour of the model, and needs to be explicitly reported.

The floating point issues in the specific fields of system biology
really point at the central issue that a model can describe reality at
several different levels of abstraction. The more detailed it is, the
more useful but also then the more special purpose (and so use-less to
many others). It is an aspect of modelling as to how abstract (versus
concrete) the model is.


%Eg., Int. Does it mean mathematical integers, C long, C long long, int32, uint64? 
%What about overflow behaviour? 
%e.g. What about random numbers?

\subsection{Protocols as scripts}

Studying the behaviours of complex models is not trivial. Whilst 
concise methods sections of papers may give a representative minimal
protocol (or workflow), missing details may present barriers to 
the reproduction. This is exacerbated by the inclusion of manual 
transformation steps which may subtly change the model. Data format 
conversions may be non-trivial or performed manually. These may 
involve external scripts, which might not be part of any of the
explicitly shared code. This can be supported by open protocols, stored 
in electronic lab notebooks during the process of model building. However
even in this case, assumed knowledge may prevent simple replicates.
%Even better would be tools which aid the transformation of text based
%protocols into real scripts.

One common approach to tackling complex protocols (or workflows) 
in computational sciences is to automate the process by scripting
the laborious elements. In its initial steps, Sidekick~\cite{Hall2014Sidekick}
builds an initial model of a $\alpha$-helical peptide, performs an
energy minimisation of the peptide \emph{in vacuo}, solvates the peptide,
adds counter-ions, and runs a second energy minimisation. This is all
done without any user actions, and the subsequent replicates are performed
with different random seeds to collect accurate statistics. Even in this 
ideal case however, variations may arise between replicates. In testing 
on a mixed AMD/Intel cluster, it was found that the solvation step added
% SI: "it was found" by who? Citation? 
a variable number of water molecules. In a molecular dynamics simulation, 
this is sufficient to cause two simulations with otherwise identical 
starting states to diverge over time. As such, the inherent properties
of the model and simulation should be taken into account in the overall
protocol design, and noted in any attempts to reproduce the behaviour.

In several disciplines, electronic lab notebooks have become the
norm. These tools, combined with open repositories e.g. FigShare,
ZappyLab, facilitate the sharing of protocols. This may be needed for
legal compliance (e.g. drug trials), but has been successfully used in
large research consortia, for example the use of ConturELN by the
structural genomics consortium in Oxford. Similarly, ZappyLab aims to
build a free, standardised protocol repository for the life
sciences. Within computational sciences, efforts to mine these
repositories could offer the potential to convert manual protocols and
work flows into prototype scripts, to aid reproducibility.

Eg Codalab

\subsection{Performance and scalability}

A key question in reproducing research in a computational context is 
whether performance is a key issue. For models of the physical world, 
such as computational fluid dynamics and molecular dynamics, it is the 
resulting physics that is typically important, rather than how fast it took to 
solve the computational problem. In algorithms research, performance can be
the key research result, and therefore reproducing this is important. Another 
example is in high performance computing where scalability of code (e.g. gromacs,
NAMD, Desmond). Here the aim is to make simulations run more efficiently
over large numbers of cores/nodes. 

If speed is not being explicitly tested, on-demand cloud resources
offer a route to reproducing computational experiments, including
using virtual machines. This provides a consistent, available channel
for distributing computational data and processing stacks together
with the infrastructure for researchers to redeploy and rerun
experiments easily. It includes repositories such as the Microsoft
Azure VMDepot\footnote{\url{http://vmdepot.msopentech.com/}} to allow
discovery and hosting of VM images online.

For research  where performance and scalability are important results, then 
reproducing these in a meaningful way can be challenging. Providing source
code, makefiles, input data and scripts is a minimum, but questions over the
testing platform must be addressed. A key question is which performance 
metric to use. Wall clock time is commonly used, but this does not allow for 
long-term performance reproducibility as any such benchmarking is a 
snapshot in time. This is true whether this is using physical 
or virtual hardware. 


% SI to say something here. 
Other structural properties like termination, stability, etc. 

Making appropriate choices on the metrics of 
performance is critical, such as execution time, operation count, and 
algorithmic efficiency.



\subsection{Benchmark repositories and annotation}

%Tom section
Benchmarks presented in scientific papers are essentially
meaningless. If every paper has to be novel, then every benchmark,
too, will be novel; there is no monotonic, historical truth in new,
synthetically-crafted benchmarks. It is worse than that really: enough
benchmarks are included to beat other tools. The comparisons are never
fair (neither are other peoples' comparisons against your tool).  The
benchmarks the tool describes are fashioned only for this instance:
they might claim to be from the Windows device driver set, but the
reality is that they are are stripped down versions of the
originals. Stripped down so much as to be useless to anyone but the
author vs. the referees. Furthermore, the cost of hardware and system
artefacts are generally overlooked~\cite{brain+devos:2009}.

Benchmarks should be public and stored in central repositories: treat
benchmark sets as a curated set of unit tests.They should allow anyone
to contribute, implying that the tests are in a standard
format. Papers should be penalised if they do not use these public
benchmarks. We need an open API for benchmarks, with standard
repositories that allow referencing and annotation. Every
test/assertion should be justified.

%\subsection{Composability and unit testing}
There is also the issue of how gross or un-breakable the benchmarks are. 
The concept of "composability" is a very fundamental one in computer science. 
That $h = f;g : A \rightarrow C$ and $f : A \rightarrow B$ and $g : A \rightarrow C$ 
in some category with structure means that, if $f$ has property $P$ and $g$ has
property $Q$, then $h$ has property $P \oplus Q$, where $\oplus$ is some 
combinatorial operator in the domain of discourse. What this abstract 
characterisation means is that a program can be tested by testing its parts,
whole system testing can be done by unit testing. 

But this is only the case if the system can be decomposed, and we know
that in many important areas like machine learning, computational
science, and CFD, the models are not often decomposable.  We have not
been explicit about it -- people normally are not, especially for
models -- but we have been discussing algorithms and models assuming
that they are very gross objects. We have not required them to be
composable or de-composable. What can this mean practically? If we
have an algorithm $A$ that claims to run on model $M$ with result $R$,
then there's no reason to assume that a slight modification of $A$
will also have result $R$. Or that $A$ running on a $M * N$, for some
operator $*$, has a suitably extended $R$ result.

We need to be very careful --- when algorithms are running on models --- when
they are running automatically, asynchronously (or due to events beyond our
control), on a global scale, with an effect like performance results that
matter to third-parties. Both algorithms and  models will need careful
curating.

A good example of some of these points is the RCSB Protein Data
Bank\footnote{\url{http://www.pdb.org}} and Systems Biology Markup
Language~\cite{Hucka2003,Chaouiya2013}. The software ones we know of,
such as the UCI Machine Learning
Repository\footnote{\url{http://archive.ics.uci.edu/ml/}}, SMT
Competition\footnote{\url{http://smtcomp.sourceforge.net/2014/}},
SV-COMP\footnote{\url{http://sv-comp.sosy-lab.org/2015/}} and the
Answer Set Programming
Competition\footnote{\url{https://www.mat.unical.it/aspcomp2014/}} are
on that journey. Such repositories would allow the tests to be taken
and easily analysed by any competitor tool. There has been work
towards developing infrastructure: for example knowledge management
systems to preserve and share whole auto-tuning and machine learning
setups for optimisation, with all related artifacts and their software
and hardware dependencies besides just performance data
~\cite{fursin-et-al:2014}.


e.g. real-world validation - AIAA drag benchmarks, ERCOFTAC
	Possibly CAPRI/CASP? (protein interaction and folding competitions)

e.g. Netflix competition (http://www.netflixprize.com/)


\section{Continuous Integration for models}

%Samin section 

The whole premise of this paper is that Algorithms (implementations)
and Models (benchmarks) go together: algorithms are designed for
certain types of models; models, though created to mimic some physical
reality, also serve to stress the current known algorithms. A
cloud-based service can make this link explicit.

Wind back several decades, and no one would {\texttt{cvs commit}} to a
serious project without running the smoke tests. Later, you could be
cleverer and run the tests via the VCS's pre-commit hook. That way
you'd never forget to run the tests. All of this can be done, at
scale, on the cloud now. Services such as Jenkins, Visual Studio
Online, etc schedule the tests to run as soon as you commit. Suppose
we have the publicly accessible benchmarks. It seems a small step to
hook these Continuous Integration (CI) systems up to the algorithm
implementations that are written to run on these benchmarks.

Suppose you have come up with a better algorithm to deal with some of
these benchmarks. You write up the paper on the algorithm but, more
importantly, you also register the implementation of your algorithm at
the service, as a possible algorithm to run on this benchmark set.

The benchmarks live in distributed git repositories. Some of the
servers that house these repos are CI servers. Now, when you push a
commit to your algorithm, or someone else pushes a commit to theirs,
or when someone else adds a new benchmarks, the service's CI system is
activated. It is also activated when a new library, firmware upgrade,
new API change, etc happens too. All registered algorithms are run on
all registered models, and the results are published. The CI servers
act as the kernel.org, the authoritative source, of results for these
algorithms running on these benchmarks.

There are already several web services that nearly do all of this. A
service that is just a bit more finished, that ties up the loose ends,
would mean that algorithms and model would evolve together, and always
be reproducible.






\section{Future outlook}

A system as described here has several up front benefits- it link papers 
more closely to their outputs, making external validation easier and 
allowing interested users to explore unaddressed sets of models. These 
features alone will be expected to accelerate the research cycle, with
major benefits. \emph{4th paradigm}~\cite{hey:2009}.

Limitations presented by big data- will some models resist validation?

Critical success factor is providing capabilities that help researchers 
in their day-to-day work, rather than being an overhead incurred, thus 
providing both immediate and long-term returns on time to the individual.

% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
% \IEEEtriggeratref{37}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}


% BibTeX users
\bibliographystyle{IEEEtran}
\bibliography{recomp2014}

\end{document}
