\documentclass[conference]{IEEEtran}

\usepackage[british]{babel}
\usepackage[hyphens]{url}
\usepackage{graphicx}
\usepackage[noadjust]{cite}
\usepackage[pdftex,colorlinks=true]{hyperref}

\begin{document}
\bstctlcite{IEEEexample:BSTcontrol}

% still not happy with the title...
%\title{``Reproducibility-as-a-Service'': Dream or Reality?}
\title{On Usefully Publishing Scientific Models} 

% author names and affiliations
% use a multiple column layout for up to three different
% affiliations
\author{\IEEEauthorblockN{Tom Crick}
\IEEEauthorblockA{Department of Computing\\
Cardiff Metropolitan University\\
Cardiff, UK\\
Email: {\url{tcrick@cardiffmet.ac.uk}}}
\and
\IEEEauthorblockN{Benjamin A. Hall and Samin Ishtiaq and Kenji Takeda}
\IEEEauthorblockA{Microsoft Research\\
Cambridge, UK\\
Email: {\url{{benhall,samin.ishtiaq,kenji.takeda}@microsoft.com}}}}

\maketitle

\begin{abstract}
Abstract here...
\end{abstract}

\IEEEpeerreviewmaketitle

\section{Introduction- reproducibility in the compuational sciences}
Two key types of results arise from work done in the computational sciences; models 
and algorithms. Models represent an abstraction of reality, and their behaviour is
expected to reliably reproduced if different algorithms are used. This validation
of a model's behaviour can be impacted by a number of factors relating to the specific
techniques used, but similar approaches are expected to give the same broad results. 
In contrast, when new algorithms are proposed to replace or supplement existing 
algorithms, they are expected to verifiably replicate the results of other algorithms.

Neither class of result though exists in isolation; a new algorithm is dependent on 
a set of models (or benchmarks) to demonstrate its new capabilities. Equally, model 
development can both necessitate the development of new algorithms and highlight the 
differences between alternative approaches. Whilst algorithms and their implementations
have been highlighted as a potential barrier to reproducibility~\cite{crick-et-al_wssspe2},
here we discuss the value of improved access and sharing of models in avoiding mistakes
and in generating new scientific insights. We describe efforts to reproduce computational 
models and algorithms, specifically the many issues related to benchmarks of models and 
algorithms. We conclude with thoughts on where efforts should be fcussed in both the short- 
and long-term to move to a world in which computational reproducibility helps researchers 
achieve their goals, rather than being perceived as an overhead.

% there are a lot of Rs in this domain:
% Replacing the Paper: The Twelve Rs of the e-Research Record~\cite{deroure:2010}
% http://www.scilogs.com/eresearch/replacing-the-paper-the-twelve-rs-of-the-e-research-record/

\section{Motivation}
The reproduction and replication of reported scientific results is a
widely discussed topic within the scientific community~\cite{barnes:2010,morin-et-al:2012,joppa-et-al:2013}. 
Whilst the retraction of several studies has drawn the focus of many
commenters, automated systems, which allow easy reproduction of results, offer the 
potential to improve the efficiency of scientific exploration and drive the
adoption of new techniques. 

New tools for membrane protein simulation~\cite{Stansfeld,hall2014} demonstrate
how complex workflows can be automated, preventing errors and differences 
arising from manual execution, whilst making it faster to perform new 
analyses. 
Alongside this, recent work in executable biology~\cite{Cook} showed how
a new class of models, representing a defined property of biological networks,
defeated an existing algorithm for proving stability. As such, it was the 
broader application of a new algorithm to additional models (or benchmarks)
which highlighted an unexplored but important phenomena the algorithm could 
not address.

Whilst experimental work requires detailed protocol descriptions, computer
science and computational science disciplines uniquely are
able to share the raw outputs of their work as software and standard format datafiles. Despite
this advantage, and alongside ongoing -- and significant -- changes to the
traditional models of academic dissemination and
publication~\cite{deroure:2010,stodden-et-al:2013,fursin+dubach:2014},
there remain cultural and technical barriers to both the
sharing and reimplementation of
algorithms~\cite{crick-et-al_wssspe2}. These include widely discussed
topics, such as choices of programming language and licence, as well
as unreported technical details of the implementations and ensuring
that developers get credit for their work.

% further to #overlyhonestmethods...
% http://www.phdcomics.com/comics.php?f=1689

However, even when this is considered, the testing of the benchmarks
themselves may be non-trivial. Benchmarks may be tailored to the 
specific problems being addressed by the algorithm, and may not be 
expected to cover all edge cases. Implementation details, such as 
floating point rounding behaviour may change benchmark results.
Furthermore, for high performance computing applications, repeating
benchmark results may not be possible by other groups without equivalent
resources. Here we extend a previous proposal~\cite{crick-et-al_wssspe2}
to specifically discuss the role of benchmark sets and analysis.

Quote: ``at least 50\% of published studies, even those in top-tier
academic journals, can't be repeated with the same conclusions by an
industrial lab''~\cite{osherovich:2011}.

%In the cloud world, we have IaaS, PaaS and SaaS...do we need RaaS: ``Reproducibility-as-a-Service''?

%SI/Ben to work on this

\section{Roads to reproducibility}

% A section about the ``How'', and the barriers to the How. 

Need to inline these examples through out the section: 
	c.f. Codalab, Kaggle

	e.g. real-world validation - AIAA drag benchmarks, ERCOFTAC
	Possibly CAPRI/CASP? (protein interaction and folding competitions)


\subsection{Implementation or model?}
%Ben/Samin section 

Specify details such as rounding behaviour in the model.

Include other examples of how abstract (vs concrete) the model should/could be. 

% Explicitly a ``protocol'' â€“ explicit steps, copy-paste, customisable,
% versioned; not black box.

% No requirement for computational expertise or significant
% computational hardware.


\subsection{Protocols as scripts}

%Ben/Samin section

'protocol' = 'work-flow'. 

Benchmarking is not always as trivial as grab some code, grab the 
existing data, and run. Data format conversions may be non-trivial
or performed manually. These may involve external scripts, which 
might not be part of any of the explicitly shared code. Open 
protocols may aid the reproducibility of benchmark results. Even 
better would be tools which aid the transformation of text based
protocols into real scripts.

Eg Codalab

Ben's example: get original model, do energy minimiztion, do some other transform, do energy minimization again. 

electronic lab books. Can't delete any of your notes. Oxford structural genomics labbooks. 

\subsection{Performance and scalability}

%Kenji to work on this

{How to assess performance?}

One big target for HPC users is scalability of code (e.g. gromacs,
NAMD, Desmond). Here the aim is to make simulations run more efficiently
over large numbers of cores/nodes. Scaling results however may be 
hard to reproduce without access to significant resource.

If speed isn't being explicitly tested, on-demand cloud resources 
may offer a route to testing small systems. Slow VMs in the cloud
may test scalability properties of algorithm without requiring access
to always on-resource.

SI: I wonder also if cloud providers can give some metric of what
load/latency/resources your job was run under.  That way, you could
calculate how your performance would scale up had you access to the
full resources. 


Wallclock vs op count

Other structural properties like termination, stability, etc. 

\subsection{Benchmark repositories and annotation}

%Tom section 

Benchmarks as unit tests, stored in central repositories. 
Treat benchmark sets as a curated set of unit tests

Eg, SMTComp, SMT Lib. 

e.g. real-world validation - AIAA drag benchmarks, ERCOFTAC
	Possibly CAPRI/CASP? (protein interaction and folding competitions)

An API for benchmarks? 

Perhaps cite Fursin et al.~\cite{fursin-et-al:2014} here as an example?

Tom: 
example of memory profiling of CLP programs. Specialist solvers versus 
generic solvers SMT have papers run on different benchmarks, testing different 
things. 

netflix competition. 

\section{CI for models}

%Samin section 

The whole premiss of this paper is that Algorithms (implementations)
and Models (benchmarks) go together: algorithms are designed for
certain types of models; models, though created to mimic some physical
reality, also serve to stress the current known algorithms. A
cloud-based service can make this link explicit.

Wind back several decades, and no one would ``cvs commit'' to a
serious project without running the smoke tests. Later, you could be
cleverer and run the tests via the VCS's pre-commit hook. That way
you'd never forget to run the tests. All of this can be done, at
scale, on the cloud now. Services such as Jenkins, Visual Studio
Online, etc schedule the tests to run as soon as you commit. Suppose
we have the publicly accesssible benchmarks. It seems a small step to
hook these Continuous Integration (CI) systems up to the algorithm
implementations that are written to run on these benchmarks.

Suppose you have come up with a better algorithm to deal with some of
these benchmarks. You write up the paper on the algorithm but, more
importantly, you also register the implementation of your algorithm at
the service, as a possible algorithm to run on this benchmark set.

The benchmarks live in distributed git repositories. Some of the
servers that house these repos are CI servers. Now, when you push a
commit to your algorithm, or someone else pushes a commit to theirs,
or when someone else adds a new benchmarks, the service's CI system is
activated. It is also activated when a new library, firmware upgrade,
new API change, etc happens too. All registered algorithms are run on
all registered models, and the results are published. The CI servers
act as the kernel.org, the authoritative source, of results for these
algorithms running on these benchmarks.

There are already several web services that nearly do all of this. A
service that is just a bit more finished, that ties up the loose ends,
would mean that algorithms and model would evolve together, and always
be reproducible.

\subsection{Composibility and unit testing}

%Samin section 
% SI: possibly this should go at the end of the Benchmarks section. 

The concept of "composability" is a very fundamental one in computer science. 
That $h = f;g : A \rightarrow C$ and $f : A \rightarrow B$ and $g : A \rightarrow C$ 
in some category with structure means that, if $f$ has property $P$ and $g$ has
property $Q$, then $hj$ has property $P \oplus Q$, where $\oplus$ is some 
combinatorial operator in the domain of discourse. What this abstract 
characterization means is that a program can be tested by testing it's parts,
whole system testing can be done by unit testing. 

But this is only the case if the system can be decomposed, and we know that in
many important areas like machine learning, computational science, and CFD,
the models aren't often decomposable.  We haven't been explicit about it ---
people normally aren't, especially for models ---  but we have been discussing
algorithms and models assuming that they are very gross objects. We haven't
required them to be composable or de-composable. What can this mnean
practically? If we have an algorithm $A$ that claims to  run on model $M$ with
result $R$, then there's no reason to assume that a slight modification of $A$
will also have result $R$. Or that $A$ running on a $M * N$, for some operator
$*$, has a suitably extended $R$ result.

We need to be very careful --- when algorithms are running on models --- when
they are running automatically, asynchronously (or due to events beyond our
control), on a global scale, with an effect like performance results that
matter to third-parties. Both algorithms and  models will need careful
curating.





\section{Future outlook}

A system as described here has several up front benefits- it link papers 
more closely to their outputs, making external validation easier and 
allowing interested users to explore unaddressed sets of models. These 
features alone will be expected to accelerate the research cycle, with
major benefits. \emph{4th paradigm}.

Limitations presented by big data- will some models resist validation?

Critical success factor is providing capabilities that help researchers 
in their day-to-day work, rather than being an overhead incurred, thus 
providing both immediate and long-term returns on time to the individual.

% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
% \IEEEtriggeratref{37}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}


% BibTeX users
\bibliographystyle{IEEEtran}
\bibliography{recomp2014}

\end{document}
