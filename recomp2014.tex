\documentclass[conference]{IEEEtran}

\usepackage[british]{babel}
\usepackage[hyphens]{url}
\usepackage{graphicx}
\usepackage[noadjust]{cite}
\usepackage[pdftex,colorlinks=true]{hyperref}
\IEEEoverridecommandlockouts

\begin{document}
\bstctlcite{IEEEexample:BSTcontrol}

%\title{``Reproducibility-as-a-Service'': Dream or Reality?}
%\title{On Usefully Publishing Scientific Models} 
%\title{Learning to share: usefully publishing scientific models}
%\title{Learning to share: publishing useful scientific models}
\title{``Share and Enjoy''\\Publishing Useful and Usable Scientific Models}

% author names and affiliations
% use a multiple column layout for up to three different
% affiliations
\author{\IEEEauthorblockN{Tom Crick}
\IEEEauthorblockA{Department of Computing\\
Cardiff Metropolitan University\\
Cardiff, UK\\
Email: {\url{tcrick@cardiffmet.ac.uk}}}%
\thanks{The authors would like to acknowledge the use of the Sirius Cybernetics
  Corporation's motto for the title of this paper. }
\and
\IEEEauthorblockN{Benjamin A. Hall and Samin Ishtiaq and Kenji Takeda}
\IEEEauthorblockA{Microsoft Research\\
Cambridge, UK\\
Email: {\url{{benhall,samin.ishtiaq,kenji.takeda}@microsoft.com}}}}

\maketitle

\begin{abstract}
% lock: Tom 
The reproduction and replication of reported scientific results is a
widely discussed hot topic within the academic community. The
retraction of several studies (including from a number of high-profile
scientists) has drawn the focus of many commentators, but there is a
wider socio-cultural problem that pervades the scientific community.

Automated systems, which allow easy reproduction of results, offer the
potential to incentivise a culture change and drive the adoption of
new techniques to improve the efficiency of scientific exploration. In
this paper, we discuss the value of improved access and sharing of the
two key types of results arising from work done in the computational
sciences: models and algorithms. We propose the development of an
integrated cloud-based system underpinning computational science,
linking together software, toolchains, workflows and outputs,
providing a seamless automated infrastructure for the verification and
validation of scientific models and in particular, performance
benchmarks.
\end{abstract}

% keywords
\begin{IEEEkeywords}
Reproducibility, Benchmarks, Models, Cloud Services,
Computational Science, Open Science
\end{IEEEkeywords}

\IEEEpeerreviewmaketitle

\section{Introduction}
% lock: Ben 
Two key types of results arise from work done in the computational
sciences: models and algorithms. Models represent an abstraction of
reality, and their behaviour is expected to be reliably reproduced if
different algorithms are used. This validation of a model's behaviour
can be impacted by a number of factors relating to the specific
techniques used, but similar approaches are expected to give broadly
the same results.  In contrast, when new algorithms are proposed to
replace or supplement existing algorithms, they are expected to
verifiably replicate the results of other algorithms.

However, neither class of result exists in isolation: a new algorithm is
dependent on a set of models (or benchmarks) to demonstrate its new
capabilities. Equally, model development can both necessitate the
development of new algorithms and highlight the differences between
alternative approaches. Whilst algorithms and their implementations
have been highlighted as a potential barrier to
reproducibility~\cite{crick-et-al_wssspe2}, in this paper we discuss the value
of improved access and sharing of models in avoiding mistakes and in
generating new scientific insights. We describe efforts to reproduce
computational models and algorithms, specifically the multitude of issues
related to benchmarks of models and algorithms. 
We conclude with thoughts % SI: are we doing this yet? 
on where efforts should be focused in both the short- and
long-term to move to a world in which computational reproducibility
helps researchers achieve their goals, rather than being perceived as
an overhead.


%\section{Motivation}
The reproduction and replication of reported scientific results is a
widely discussed topic within the scientific
community~\cite{barnes:2010,morin-et-al:2012,joppa-et-al:2013}.
Whilst the retraction of several studies has drawn the focus of many
commentators, automated systems, which allow easy reproduction of
results, offer the potential to improve the efficiency of scientific
exploration and drive the adoption of new techniques. Nevertheless,
this is wider socio-cultural problem that pervades the scientific
community, with estimates that as much as 50\% of published studies,
even those in top-tier academic journals, cannot be repeated with the
same conclusions by an industrial lab~\cite{osherovich:2011}.

Specific examples of the benefits of reproducible workflows to researchers 
are now appearing in the scientific literature. New tools for membrane protein simulation~\cite{Stansfeld,Hall2014}
demonstrate how complex workflows can be automated, preventing errors
and differences arising from manual execution, whilst making it faster
to perform new analyses. More complex tools, such as Copernicus, aim to 
automate more generic molecular dynamics workflows~\cite{Pronk}.
Alongside this, recent work in executable
biology~\cite{Cook} showed how a new class of models, representing a
defined property of biological networks, defeated an existing
algorithm for proving stability. As such, it was the broader
application of a new algorithm to additional models (or benchmarks)
which highlighted an unexplored but important phenomena the algorithm
could not address.

%Whilst experimental work requires detailed protocol descriptions,
%computer science and the computational science disciplines uniquely
%are able to share the raw outputs of their work as software and
%standard format datafiles. 
In each case, these tools take advatage of a fundemental advantage
of computer science and computational science- the unique ability to
share the raw outputs of their work as software and datafiles. However, 
despite this advantage, and alongside
ongoing -- and significant -- changes to the traditional models of
academic dissemination and
publication~\cite{deroure:2010,stodden-et-al:2013,fursin+dubach:2014},
there remain cultural and technical barriers to both the sharing and
reimplementation of algorithms~\cite{crick-et-al_wssspe2}. These
include widely discussed topics, such as choices of programming
language and licence, as well as unreported technical details of the
implementations and ensuring that developers get credit for their
work\footnote{e.g. UK Community of Research Software Engineers:
  ~\url{http://www.rse.ac.uk/}}.

% further to #overlyhonestmethods...
% http://www.phdcomics.com/comics.php?f=1689

\begin{figure}[!ht]
\centering
\includegraphics[width=\columnwidth]{phd031214s.png}
\caption{Reproducible research\newline [source: \url{http://www.phdcomics.com/comics.php?f=1689}]}
\label{fig:reprodres} 
\end{figure}

However, even when this is considered, the testing of the benchmarks
themselves may be non-trivial. Benchmarks may be tailored to the
specific problems being addressed by the algorithm, and may not be
expected to cover all edge cases. Implementation details, such as
floating point rounding behaviour, may affect benchmark results.
Furthermore, for high performance computing applications, repeating
benchmark results may not be possible by other groups without
equivalent hardware resources. Here we extend a previous
proposal~\cite{crick-et-al_wssspe2} by specifically discussing the 
problems posed by models, considering the issues surrounding sharing
and analysing benchmark sets.



\section{The nature of models}

\subsection{Abstraction levels} 

A model describes reality at some level of abstraction. The more
detailed it is, the more faithful it is, but also then the more
special purpose (and so use-less to many others). It is an important
aspect of the modelling task as to what level (abstract versus
concrete) to model at. But often it is implicit, embodied but not
embedded, in the model.

% Published models require basic information to allow the models to be
% explored. On the most basic level, this can include techniques to
% allow files to be parsed, but in this parsing errors may arise from
% details stored in the original implementations of the model analysis
% algorithms.  
An example of this comes from the treatment of floating point
conversions in qualitative networks in systems
biology~\cite{Schaub2007}. Each variable in a network has an algebraic
target function which describes how the variable should change at each
step. The variables themselves are integers, and the target function
may return a float, which must be converted to an integer for the
update. This can be done in the target function itself, but if the
function returns a float, the specific implementation dictates if this
is a round, floor or ceil function. This implicit (to the model) but
specific (from the implementation) may change the results of the
modelling.

Another example showing how the implementation of qualitative networks
may change the model is the treatment of variable ranges within the
model.  Whilst the formalism allows the variables in a model to have
differing ranges of integers, the mechanism of conversion is not
specified in the formalism. As such, this is another area where
implementations can dictate the precise behaviour of the model, and
needs to be explicitly annotated on the model.


%Eg., Int. Does it mean mathematical integers, C long, C long long, int32, uint64? 
%What about overflow behaviour? 
%e.g. What about random numbers?

\subsection{Benchmark repositories and annotation}

%Tom section
% Benchmarks presented in scientific papers are essentially
% meaningless. If every paper has to be novel, then every benchmark,
% too, will be novel; there is no monotonic, historical truth in new,
% synthetically-crafted benchmarks. It is worse than that really: enough
% benchmarks are included to beat other tools. The comparisons are never
% fair (neither are other peoples' comparisons against your tool).  The
% benchmarks the tool describes are fashioned only for this instance:
% they might claim to be from the Windows device driver set, but the
% reality is that they are are stripped down versions of the
% originals. Stripped down so much as to be useless to anyone but the
% author vs. the referees. 

A model, or a test, needs to be annotated. A benchmark is a set of
models that have been put together for some explicit purpose. Perhaps
the directory structure of the benchmarks tells us this purpose,
perhaps assertions in the models tell us the purpose. In short, the
benchmarks needs to be curated.  If the benchmark is public (allowing
anyone to contribute), then the curation is even more necessary to
make the models reusable.

Once there is a set of tests, there is the issue of how independent or
un-breakable the set is.  The concept of "composability" is a very
fundamental one in computer science.  That $h = f;g : A \rightarrow C$
and $f : A \rightarrow B$ and $g : A \rightarrow C$ in some category
with structure means that, if $f$ has property $P$ and $g$ has
property $Q$, then $h$ has property $P \oplus Q$, where $\oplus$ is
some combinatorial operator in the domain of discourse. What this
abstract characterisation means is that a program can be tested by
testing its parts, whole system testing can be done by unit testing.

But this is only the case if the system can be decomposed, and we know
that in many important areas like machine learning, computational
science, and CFD, the models are not often decomposable.  We have not
been explicit about it -- people normally are not, especially for
models -- but we have been discussing algorithms and models assuming
that they are very gross objects. We have not required them to be
composable or de-composable. What can this mean practically? If we
have an algorithm $A$ that claims to run on model $M$ with result $R$,
then there is no reason to assume that a slight modification of $A$
will also have result $R$. Or that $A$ running on a $M * N$, for some
operator $*$, has a suitably extended $R$ result.

We need to be very careful -- when algorithms are running on models --
when they are running automatically, asynchronously (or due to events
beyond our control), on a global scale, with an effect like
performance results that matter to third-parties. Both algorithms and
models will need careful curating.

Some good examples of such benchmarks are the UCI Machine Learning
Repository\footnote{\url{http://archive.ics.uci.edu/ml/}}, the Netflix
competition benchmarks\footnote{\url{http://www.netflixprize.com/}},
SMT Competition\footnote{\url{http://smtcomp.sourceforge.net/2014/}},
SV-COMP\footnote{\url{http://sv-comp.sosy-lab.org/2015/}}, the
Answer Set Programming
Competition\footnote{\url{https://www.mat.unical.it/aspcomp2014/}},
and the Termination Problem Database 
\footnote{\url{http://termination-portal.org/wiki/TPDB}} are
on that journey. Such repositories would allow the tests to be taken
and easily analysed by any competitor tool. There has been work
towards developing infrastructure: for example knowledge management
systems to preserve and share whole auto-tuning and machine learning
setups for optimisation, with all related artifacts and their software
and hardware dependencies besides just performance data
~\cite{fursin-et-al:2014}.


\section{Workflow of meta-models}

\subsection{Protocols as scripts}

Studying the behaviours of complex models is not trivial. Whilst 
concise methods sections of papers may give a representative minimal
protocol (or workflow), missing details may present barriers to 
the reproduction. This is exacerbated by the inclusion of manual 
transformation steps which may subtly change the model. Data format 
conversions may be non-trivial or performed manually. These may 
involve external scripts, which might not be part of any of the
explicitly shared code. This can be supported by open protocols, stored 
in electronic lab notebooks during the process of model building. However
even in this case, assumed knowledge may prevent simple replicates.
%Even better would be tools which aid the transformation of text based
%protocols into real scripts.

One common approach to tackling complex protocols (or workflows) in
computational sciences is to automate the process by scripting the
laborious elements. In its initial steps,
Sidekick~\cite{Hall2014Sidekick} builds an initial model of a
$\alpha$-helical peptide, performs an energy minimisation of the
peptide \emph{in vacuo}, solvates the peptide, adds counter-ions, and
runs a second energy minimisation. This is all done without any user
actions, and the subsequent replicates are performed with different
random seeds to collect accurate statistics. Even in this ideal case
however, variations may arise between replicates. In testing on a
mixed AMD/Intel cluster, one of the authors found that the solvation
step added a variable number of water molecules. In a molecular
dynamics simulation, this is sufficient to cause two simulations with
otherwise identical starting states to diverge over time. As such, the
inherent properties of the model and simulation should be taken into
account in the overall protocol design, and noted in any attempts to
reproduce the behaviour.

In several disciplines, electronic lab notebooks have become the
norm. These tools, combined with open repositories e.g. FigShare,
ZappyLab, facilitate the sharing of protocols. This may be needed for
legal compliance (e.g. drug trials), but has been successfully used in
large research consortia, for example the use of ConturELN by the
structural genomics consortium in Oxford. Similarly, ZappyLab aims to
build a free, standardised protocol repository for the life
sciences. Within computational sciences, efforts to mine these
repositories could offer the potential to convert manual protocols and
work flows into prototype scripts, to aid reproducibility.

\subsection{Performance and scalability}

A key question in reproducing research in a computational context is
whether performance is a key issue. For models of the physical world,
such as computational fluid dynamics and molecular dynamics, it is the
resulting physics that is typically important, rather than how fast it
took to solve the computational problem. In algorithms research,
performance can be the key research result, and therefore reproducing
this is important. Another example is in high performance computing
where scalability of code (e.g. gromacs, NAMD, Desmond). Here the aim
is to make simulations run more efficiently over large numbers of
cores/nodes. On-demand cloud resources like Amazon spot market
instances offer a route to reproducing computational experiments.

A key question is which performance metric to use. Wall clock time is
commonly used, but this does not allow for long-term performance
reproducibility as any such benchmarking is a snapshot in time. This
is true whether the underlying hardware the software is running on is
physical or virtual hardware. Some ``op count'' is a more interesting
measure. Also, other structural properties of the models the
algorithms are running on, are more interesting. In the field of
system biology, whether an algorithm can prove properties like
termination, stability, interesting start conditions, etc, are useful
measurements of whether one algorithm is better than
another. Similarly, in Answer Set Programming, the cost of hardware
and system artefacts are important but generally overlooked
(\cite{brain+devos:2009} is a good example of considering them).



\section{Future outlook}


% Limitations presented by big data- will some models resist validation?

% Critical success factor is providing capabilities that help researchers 
% in their day-to-day work, rather than being an overhead incurred, thus 
% providing both immediate and long-term returns on time to the individual.

%\section{Continuous Integration for models}

The whole premise of this paper is that Algorithms (implementations)
and Models (benchmarks) go together: algorithms are designed for
certain types of models; models, though created to mimic some physical
reality, also serve to stress the current known algorithms. A
cloud-based service can make this link explicit.

Wind back several decades, and no one would {\texttt{cvs commit}} to a
serious project without running the smoke tests. Later, you could be
cleverer and run the tests via the VCS's pre-commit hook. That way
you'd never forget to run the tests. All of this can be done, at
scale, on the cloud now. Services such as Jenkins, Visual Studio
Online, etc schedule the tests to run as soon as you commit. Suppose
we have the publicly accessible benchmarks. It seems a small step to
hook these Continuous Integration (CI) systems up to the algorithm
implementations that are written to run on these benchmarks.

Suppose you have come up with a better algorithm to deal with some of
these benchmarks. You write up the paper on the algorithm but, more
importantly, you also register the implementation of your algorithm at
the service, as a possible algorithm to run on this benchmark set.

The benchmarks live in distributed git repositories. Some of the
servers that house these repos are CI servers. Now, when you push a
commit to your algorithm, or someone else pushes a commit to theirs,
or when someone else adds a new benchmarks, the service's CI system is
activated. It is also activated when a new library, firmware upgrade,
new API change, etc happens too. All registered algorithms are run on
all registered models, and the results are published. The CI servers
act as the kernel.org, the authoritative source, of results for these
algorithms running on these benchmarks.

There are already several web services that nearly do all of this. A
service that is just a bit more finished, that ties up the loose ends,
would mean that algorithms and model would evolve together, and always
be reproducible.


A system as described here has several up front benefits- it link papers 
more closely to their outputs, making external validation easier and 
allowing interested users to explore unaddressed sets of models. These 
features alone will be expected to accelerate the research cycle, with
major benefits. \emph{4th paradigm}~\cite{hey:2009}.

% BibTeX users
\bibliographystyle{IEEEtran}
\bibliography{recomp2014}

\end{document}
