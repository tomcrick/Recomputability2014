\documentclass[conference]{IEEEtran}

\usepackage[british]{babel}
\usepackage[hyphens]{url}
\usepackage{graphicx}
\usepackage[noadjust]{cite}
\usepackage[pdftex,colorlinks=true]{hyperref}

\begin{document}
\bstctlcite{IEEEexample:BSTcontrol}

% still not happy with the title...
\title{``Reproducibility-as-a-Service'': Dream or Reality?}

% author names and affiliations
% use a multiple column layout for up to three different
% affiliations
\author{\IEEEauthorblockN{Tom Crick}
\IEEEauthorblockA{Department of Computing\\
Cardiff Metropolitan University\\
Cardiff, UK\\
Email: {\url{tcrick@cardiffmet.ac.uk}}}
\and
\IEEEauthorblockN{Kenji Takeda, Benjamin A. Hall and Samin Ishtiaq}
\IEEEauthorblockA{Microsoft Research\\
Cambridge, UK\\
Email: {\url{{kenji.takeda,benhall,samin.ishtiaq}@microsoft.com}}}}

\maketitle

\begin{abstract}
Abstract here...
\end{abstract}

\IEEEpeerreviewmaketitle

\section{Introduction}

The reproduction and replication of reported scientific results is a 
topic of growing concern within the scientific community. Whilst
experimental work requires detailed protocol descriptions, computer
science and related disciplines uniquely are able to share the raw
outputs of their work- the software. 
Despite this advantage, there however remain cultural and technical 
barriers to both the sharing and reimplementation of 
algorithms~\cite{crick-et-al_wssspe2}. These include widely discussed
topics, such as choices of programming language and licence, as well
as unreported technical details of the implementations and ensuring 
that developers get credit for their work.

However, even when this is considered, the testing of the benchmarks
themselves may be non-trivial. Benchmarks may be tailored to the 
specific problems being addressed by the algorithm, and may not be 
expected to cover all edge cases. Implementation details, such as 
floating point rounding behaviour may change benchmark results.
Furthermore, for high performance computing applications, repeating
benchmark results may not be possible by other groups without equivalent
resources. Here we extend a previous proposal~\cite{crick-et-al_wssspe2}
to specifically discuss the role of benchmark sets and analysis.

\section{Implementation or model?}

Specify details such as rounding behaviour in the model

\section{Protocols as scripts}

Benchmarking is not always as trivial as grab some code, grab the 
existing data, and run. Data format conversions may be non-trivial
or performed manually. These may involve external scripts, which 
might not be part of any of the explicitly shared code. Open 
protocols may aid the reproducibility of benchmark results. Even 
better would be tools which aid the transformation of text based
protocols into real scripts.


\section{Performance and scalability}

One big target for HPC users is scalability of code (e.g. gromacs,
NAMD, Desmond). Here the aim is to make simulations run more efficiently
over large numbers of cores/nodes. Scaling results however may be 
hard to reproduce without access to significant resource.

If speed isn't being explicitly tested, on-demand cloud resources 
may offer a route to testing small systems. Slow VMs in the cloud
may test scalability properties of algorithm without requiring access
to always on-resource. \emph{NOTE- DOES THIS SOUND SENSIBLE?}


\section{Benchmark repositories and annotation}

Benchmarks as unit tests, stored in central repositories. An API 
for benchmarks? Treat benchmark sets as a curated set of unit tests

\section{Benchmarks as a service}

Code interacts with benchmarks in a reliable way. We want to share
code, and have proposed a mechanism for doing this~\cite{crick-et-al_wssspe2}.
Explicitly link online repositories of code to benchmark sets and
results. Allows reproduction of benchmark results from a paper, 
but also trivial expansion of benchmarking by linking to other 
available datasets.


% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
% \IEEEtriggeratref{37}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}


% BibTeX users
\bibliographystyle{IEEEtran}
\bibliography{recomp2014}

\end{document}
